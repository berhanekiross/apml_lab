{"cells":[{"cell_type":"code","execution_count":14,"id":"3fff6ea1","metadata":{"tags":[]},"outputs":[],"source":["%matplotlib inline\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torchvision import datasets, transforms\n","\n","# use GPU for computation if possible\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":15,"id":"c9146cf7","metadata":{"tags":[]},"outputs":[],"source":["# load code for NVAE\n","\n","# make sure that files in local directory can be imported\n","import sys\n","sys.path.insert(0, \".\")\n","\n","# try to import nvae module and download it if it fails\n","import urllib\n","try:\n","    import nvae\n","except ImportError:\n","    urllib.request.urlretrieve(\"https://uu-sml.github.io/course-apml-public/lab/nvae.py\", \"nvae.py\")\n","    import nvae"]},{"cell_type":"markdown","id":"c4ac19f0","metadata":{"tags":[]},"source":["# Deep hierarchical variational autoencoder\n","\n","We studied PCA, probabilistic PCA, and a simple variational autoencoder (VAE) on the MNIST data of handwritten digits in exercise 11.1 and the computer lab (links to the material can be found [here](https://uu-sml.github.io/course-apml-public/)). In this notebook we will play around with a much more complicated VAE, the so-called [nouveau variational autoencoder (NVAE)](https://arxiv.org/abs/2007.03898). It is based on an advanced deep hierarchical structure of the encoder and decoder. The authors applied it to multiple image data sets such as the MNIST data set and the [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) and [CelebA HQ](https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf) data sets of celebrity images. The image below shows samples of the NVAE for the CelebA HQ data set.\n","\n","<img src=\"https://raw.githubusercontent.com/NVlabs/NVAE/master/img/celebahq.png\" width=\"800\">"]},{"cell_type":"markdown","id":"3a1955d7","metadata":{"tags":[]},"source":["## Before you start\n","\n","It is strongly recommended that you complete the first parts of the computer lab and perform a probabilistic PCA and train a simple VAE on the MNIST data before running this notebook. You can download the notebook for the PPCA model [here](https://uu-sml.github.io/course-apml-public/lab/PPCA.ipynb) and run it on your computer, or you can [open it on Google Colab](https://colab.research.google.com/github/uu-sml/course-apml-public/blob/master/lab/PPCA.ipynb). The notebook for the VAE you can download [here](https://uu-sml.github.io/course-apml-public/lab/VAE.ipynb) or you can [open it on Google Colab](https://colab.research.google.com/github/uu-sml/course-apml-public/blob/master/lab/VAE.ipynb)\n","\n","We recommend also that you [open this notebook on Google Colab](https://colab.research.google.com/github/uu-sml/course-apml-public/blob/master/lab/NVAE.ipynb) since it allows you to use GPUs which will speed up the computations a lot (you can enable GPU support in the \"Runtime\" -> \"Change runtime type\" menu). Alternatively, you can run the notebook on your computer. In this notebook we use [PyTorch](https://pytorch.org/), an open source software library for machine learning. Make sure that you have installed the latest version of PyTorch if you run the notebook on your computer. \n","\n","A Jupyter notebook with an introduction to PyTorch can be downloaded from [here](https://uu-sml.github.io/course-sml-public/lab/introduction.ipynb). Alternatively, you can [open it on Google Colab](https://colab.research.google.com/github/uu-sml/course-sml-public/blob/master/lab/introduction.ipynb). Reading and running the notebook is highly recommended, since it introduces important concepts and commands that are required in this notebook."]},{"cell_type":"markdown","id":"f71aae4c","metadata":{"tags":[]},"source":["## Model\n","\n","The NVAE model is too large to be trained on your computer (or on Google Colab) during the lab session. Therefore we use pretrained parameters that are [made available by the authors](https://github.com/NVlabs/NVAE#post-training-sampling-evaluation-and-checkpoints). We download the parameters (the filesize is around 217MB) and construct a NVAE for the MNIST data."]},{"cell_type":"code","execution_count":16,"id":"793c2042","metadata":{"scrolled":true,"tags":[]},"outputs":[{"ename":"UnpicklingError","evalue":"invalid load key, '<'.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load pretrained NVAE for MNIST\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# move model to the GPU if available\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[1;32mc:\\Users\\Berhane\\Desktop\\Advanced Probailistic ML\\4. Lab\\nvae.py:110\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# load saved checkpoint of model\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading checkpoint of pretrained model for\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Berhane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1384\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1383\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Berhane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1628\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1623\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1624\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1626\u001b[0m     )\n\u001b[1;32m-> 1628\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '<'."]}],"source":["# load pretrained NVAE for MNIST\n","model = nvae.load_pretrained_model(\"mnist\")\n","\n","# move model to the GPU if available\n","model = model.to(device)"]},{"cell_type":"markdown","id":"b7ae5d25","metadata":{"tags":[]},"source":["We can print a summary of the network architecture."]},{"cell_type":"code","execution_count":9,"id":"857873a7","metadata":{"tags":[]},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n","\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model"]},{"cell_type":"markdown","id":"3d6c62d4","metadata":{"tags":[]},"source":["The NVAE consists of many convolutional layers which also explains the performance improvements when you run it on a GPU."]},{"cell_type":"code","execution_count":null,"id":"54cc9bc2","metadata":{"scrolled":true,"tags":[]},"outputs":[],"source":["# number of convolutional layers\n","len(model.all_conv_layers)"]},{"cell_type":"markdown","id":"71494449","metadata":{"tags":[]},"source":["In total, the NVAE consists of almost 19 million parameters."]},{"cell_type":"code","execution_count":null,"id":"8bac2b11","metadata":{"tags":[]},"outputs":[],"source":["# number of parameters\n","nvae.count_parameters(model)"]},{"cell_type":"markdown","id":"43d74834","metadata":{"tags":[]},"source":["## Reconstructions\n","\n","We check how well the NVAE can reconstruct MNIST images. The model expects the MNIST data to be padded with two white pixels at the bottom, top, left, and right side. Moreover, the model is trained with a binarized version of the MNIST data that consists only of 0s and 1s instead of values between 0 and 1. If $\\mathbf{x} = [\\mathbf{x}_1 \\cdots \\mathbf{x}_{1024}]^\\mathsf{T}$ denotes an image in the MNIST data set with padding, then a binarized version $\\mathbf{y} = [\\mathbf{y}_1 \\cdots \\mathbf{y}_{1024}]^\\mathsf{T}$ is obtained by sampling entry $\\mathbf{y}_i$ ($i = 1,\\ldots,1024$) from a Bernoulli distribution with parameter $\\mathbf{x}_i$, i.e.,\n","\\begin{equation*}\n","    \\mathbf{y}_i \\sim \\operatorname{Bern}(\\mathbf{y}_i; \\mathbf{x}_i).\n","\\end{equation*}"]},{"cell_type":"code","execution_count":null,"id":"ec8b349d","metadata":{"tags":[]},"outputs":[],"source":["# preprocessing pipeline: add padding and sample binarized version\n","preprocess = transforms.Compose([\n","    transforms.Pad(2),\n","    lambda x: x.bernoulli(),\n","])"]},{"cell_type":"markdown","id":"bbb9f2bd","metadata":{"tags":[]},"source":["The decoder of the NVAE returns images that of size 32x32, consistent with the padding of the MNIST data. We crop the images when we compare them with the MNIST data without padding."]},{"cell_type":"code","execution_count":null,"id":"e5be2bb7","metadata":{"tags":[]},"outputs":[],"source":["centercrop = transforms.CenterCrop(28)"]},{"cell_type":"markdown","id":"06089b6e","metadata":{"tags":[]},"source":["To ensure that the computation is reasonably fast also if you do not use a GPU we limit ourselves to batches of 10 random images from the test data set in our analysis."]},{"cell_type":"code","execution_count":null,"id":"c4c2abf6","metadata":{"tags":[]},"outputs":[],"source":["# download MNIST test data\n","testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n","\n","# define data loader\n","# if you use a GPU you can increase the batch size\n","# `pin_memory=True` is helpful when working with GPUs: https://pytorch.org/docs/stable/data.html#memory-pinning\n","testdata = torch.utils.data.DataLoader(\n","    testset, batch_size=10, shuffle=True, pin_memory=device.type=='cuda',\n",")"]},{"cell_type":"markdown","id":"fcf531fe","metadata":{"tags":[]},"source":["In the following code block we draw 10 random test images and compute their reconstructions, i.e., the output of the NVAE when running the encoder and the decoder."]},{"cell_type":"code","execution_count":null,"id":"165eb1cf","metadata":{"tags":[]},"outputs":[],"source":["with torch.no_grad(): # no gradients required\n","    # batch of 10 random images + labels from the test data set\n","    test_images, test_labels = next(iter(testdata))\n","    \n","    # move data to the GPU if available\n","    test_images = test_images.to(device)\n","\n","    # compute reconstructions\n","    logits = model(preprocess(test_images))[0]\n","    test_reconstructions = centercrop(model.decoder_output(logits).mean)"]},{"cell_type":"markdown","id":"f697130e","metadata":{"tags":[]},"source":["We compare the original images and their reconstructions visually."]},{"cell_type":"code","execution_count":null,"id":"b2ea7e9b","metadata":{"tags":[]},"outputs":[],"source":["# plot a grid of random pairs of `originals` and `reconstructions`\n","def plot_reconstructions(originals, reconstructions, labels, nrows=4, ncols=2):\n","    # indices of displayed samples\n","    n = originals.shape[0]\n","    indices = np.random.choice(n, size=nrows*ncols, replace=False)\n","\n","    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n","    for (idx, ax) in zip(indices, axes.flat):\n","        # extract original, reconstruction, and label\n","        original = originals[idx]\n","        reconstruction = reconstructions[idx]\n","        label = labels[idx]\n","\n","        # configure subplot\n","        ax.set_xticks(())\n","        ax.set_yticks(())\n","        ax.grid(False)\n","        ax.set_title(f\"Label: {label.item()}\", fontweight='bold')\n","\n","        # plot original and reconstructed image in a grid\n","        grid = np.ones((32, 62))\n","        grid[2:30, 2:30] = original.squeeze().cpu().numpy()\n","        grid[2:30, 32:60] = reconstruction.squeeze().cpu().numpy()\n","        ax.imshow(grid, vmin=0.0, vmax=1.0, cmap='gray_r')\n","\n","    return fig"]},{"cell_type":"code","execution_count":null,"id":"e8cfcfa4","metadata":{"tags":[]},"outputs":[],"source":["plot_reconstructions(test_images, test_reconstructions, test_labels, nrows=5)\n","plt.show()"]},{"cell_type":"markdown","id":"976eaf27","metadata":{"tags":[]},"source":["The reconstructions seem to be of much higher quality than the reconstructions from the simple VAE that we trained. This is not completely surprising though given the high complexity of the NVAE model.\n","\n","Again we also analyze the average squared reconstruction error\n","\\begin{equation*}\n","    \\mathrm{sqerr} := \\frac{1}{10000} \\sum_{i=1}^{10000} \\|\\mathbf{x}_i - \\tilde{\\mathbf{x}}_i\\|^2_2\n","\\end{equation*}\n","of the images $\\mathbf{x}_i \\in {[0,1]}^{784}$ and their reconstructions $\\tilde{\\mathbf{x}}_i \\in \\mathbb{R}^{784}$ ($i = 1,\\ldots, 10000$) in the MNIST test data set as an objective measure for the quality of the reconstructions. To reduce the computation time, in particular when you do not use a GPU, we estimate it only from 10 batches of 10 random test images and do not evaluate the full test data set. If the estimation is reasonably fast you can increase the number of batches."]},{"cell_type":"code","execution_count":null,"id":"10eb35a5","metadata":{"tags":[]},"outputs":[],"source":["# only analyze 10 out of 1000 batches\n","# you can increase this if you use a GPU\n","nbatches = 10\n","\n","sqerrors = []\n","\n","with torch.no_grad(): # no gradients required\n","    for i, (images, _) in enumerate(testdata):\n","        # only analyze nbatches batches\n","        if i >= nbatches:\n","            break\n","\n","        # show progress\n","        print(\"processing batch {:3d} ...\".format(i + 1))\n","        \n","        # move data to GPU if available\n","        images = images.to(device)\n","    \n","        # compute reconstructions\n","        logits = model(preprocess(images))[0]\n","        reconstructions = centercrop(model.decoder_output(logits).mean)\n","        \n","        # compute average squared reconstruction error\n","        sqerror = (images - reconstructions).pow(2).view(-1, 784).sum(dim=1).mean()\n","        sqerrors.append(sqerror)\n","\n","sqerr = torch.tensor(sqerrors).mean()\n","print(f\"Average squared reconstruction error: {sqerr}\")"]},{"cell_type":"markdown","id":"73462b72","metadata":{"tags":[]},"source":["## Generating images\n","\n","Of course, we can also generate new images from the NVAE. Here we sample one image from the NVAE with temperature `T` and display it."]},{"cell_type":"code","execution_count":null,"id":"e2ec9537","metadata":{"scrolled":true,"tags":[]},"outputs":[],"source":["# sample a single image with temperature `T`\n","T = 0.6\n","\n","with torch.no_grad(): # no gradients required\n","    # compute decoding distribution\n","    logits = model.sample(1, T) \n","    output = model.decoder_output(logits)\n","    \n","    # use non-binarized sample for MNIST, otherwise sample from the decoding distribution\n","    output_img = output.mean if isinstance(output, torch.distributions.bernoulli.Bernoulli) else output.sample()\n","    \n","    # reorder the axes as (nsamples, channels, height, width)\n","    output_img = output_img.permute(0, 2, 3, 1).squeeze().cpu().numpy()\n","\n","    # crop MNIST images\n","    if model.dataset == 'mnist':\n","        output_img = output_img[2:30, 2:30]\n","\n","# plot sample\n","plt.xticks([])\n","plt.yticks([])\n","if model.dataset == 'mnist':\n","    # plot MNIST images as grayscale images\n","    plt.imshow(output_img, vmin=0.0, vmax=1.0, cmap='gray_r')\n","else:\n","    plt.imshow(output_img)\n","plt.show()"]},{"cell_type":"markdown","id":"dcc03992","metadata":{"tags":[]},"source":["### Task\n","\n","Play around with the notebook. Some suggestions:\n","- Change the temperature and study how it affects the quality of the generated image.\n","- Generate images from pretrained NVAEs for other image data sets (you can not analyze the reconstructions since we did not load these other data sets).\n","\n","**Warning: The files with pretrained parameters for other data sets are large (> 1.5GB). You might have to delete some files with pretrained parameters in the file browser on Google Colab to free up space. Since the NVAEs for other image data sets are also more complex (e.g., more convolutional layers) it takes more time to run them, in particular if you do not use a GPU.**"]},{"cell_type":"code","execution_count":null,"id":"d59687e0","metadata":{"tags":[]},"outputs":[],"source":["# load pretrained NVAE\n","# possible data sets:\n","# \"mnist\", \"celeba_64\", \"celeba_256a\", \"celeba_256b\", \"cifar10a\", \"cifar10b\", \"ffhq\"\n","model = nvae.load_pretrained_model(\"mnist\")\n","\n","# move model to the GPU if available\n","model = model.to(device)"]}],"metadata":{"@webio":{"lastCommId":null,"lastKernelId":null},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
